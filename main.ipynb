{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import argparse\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports own code\n",
    "from CNN.backend import load_or_train_CNN, Net, pred_single_image\n",
    "from CNN.plotting import plot_confusion_matrix, plot_loss\n",
    "from SVD.back import visualize_svd, compute_svd_per_digit, classify_image, load_or_run_all_ranks\n",
    "import KNN.func as knn\n",
    "from CNN.plotting import visualize_activations, get_single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pathing\n",
    "path_raw_data = Path.cwd() / 'data'/ 'MNIST' / 'raw'\n",
    "path_download = Path.cwd() / 'data'\n",
    "\n",
    "path_svd = Path.cwd() / 'SVD'\n",
    "path_knn = Path.cwd() / 'KNN'\n",
    "path_CNN_results = Path.cwd() / 'CNN' / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if not already present (pytorch implementation)\n",
    "# Put in data folder earlier described\n",
    "# d1 and d2 are dummy variables besides that!\n",
    "_d1 = datasets.MNIST(path_download, train=True, download=True)\n",
    "_d2 = datasets.MNIST(path_download, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic handling the importing of images straight from the files.\n",
    "\n",
    "# Load images from the MNIST dataset\n",
    "def load_image_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Skip the magic number and number of images\n",
    "        _, num_images = struct.unpack(\">II\", f.read(8))\n",
    "        rows, cols = struct.unpack(\">II\", f.read(8))  # Image dimensions\n",
    "        # Read image data and reshape into (num_images, rows*cols)\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows * cols)\n",
    "    return {'n': num_images, 'x': images}\n",
    "\n",
    "# Load labels from the MNIST dataset\n",
    "def load_label_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Skip the magic number and number of labels\n",
    "        _, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        # Read label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Load training and test datasets\n",
    "def load_mnist(folder: Path):\n",
    "    train = load_image_file(folder / 'train-images-idx3-ubyte')\n",
    "    test = load_image_file(folder /'t10k-images-idx3-ubyte')\n",
    "    \n",
    "    train['y'] = load_label_file(folder / 'train-labels-idx1-ubyte')\n",
    "    test['y'] = load_label_file(folder / 't10k-labels-idx1-ubyte')\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Display an MNIST digit image\n",
    "def show_digit(arr784):\n",
    "    plt.imshow(arr784.reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')  # Hide axes for cleaner display\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "# Load MNIST data\n",
    "train, test = load_mnist(path_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example image (e.g., the 5th training image)\n",
    "show_digit(train['x'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the average of all digits\n",
    "\n",
    "averages = knn.average_images(train['x'], train['y'])\n",
    "imgs = [np.array(averages[0][i]) for i in range(10)]\n",
    "avg_img_array = [img.reshape(28,28) for img in imgs]\n",
    "knn.show_digits(avg_img_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster centroids for digit 3 (5 centroids)\n",
    "data = [train['x'][i] for i in range(len(train['x'])) if train['y'][i] == 3]\n",
    "kmeans = KMeans(n_clusters = 5).fit(data)\n",
    "centers = kmeans.cluster_centers_\n",
    "arr_centers = [center.reshape(28,28) for center in centers]\n",
    "\n",
    "knn.show_digits(arr_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers\n",
    "np.shape(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = load_label_file(path_raw_data / 't10k-labels-idx1-ubyte')\n",
    "test_images = load_image_file(path_raw_data /  't10k-images-idx3-ubyte')\n",
    "\n",
    "print(f\"Number of test labels: {len(test_labels)}\")\n",
    "print(f\"Number of test images: {len(test_images)}\")\n",
    "\n",
    "print(test_labels)\n",
    "\n",
    "train_labels = load_label_file( path_raw_data / 'train-labels-idx1-ubyte')\n",
    "train_images = load_image_file(path_raw_data /  'train-images-idx3-ubyte')\n",
    "train_labels\n",
    "train['y'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_svd(train['x'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example image\n",
    "for i in range(0,10):\n",
    "    visualize_svd(train['x'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load / Define model / Set up settings for the model. \n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example');\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=25, metavar='N',\n",
    "                    help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                    help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                    help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                    help='disables macOS GPU training')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                    help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10000, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                    help='For Saving the current Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An architecture overview of the model\n",
    "# For further details, check the Net class in the backend.py file (there you will see a few more details)\n",
    "_model = Net()\n",
    "summary(_model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or train the model\n",
    "# Can set force to true if you want to force it to retrain\n",
    "results, model = load_or_train_CNN(path_CNN_results, path_download, force=True, parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plot_loss(results=results, save_folder= path_CNN_results, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices of the last epoch\n",
    "max_epoch = results[\"epochs\"]\n",
    "plot_confusion_matrix(results=results, epoch=max_epoch, save_folder=path_CNN_results, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices of the best epoch\n",
    "best_epoch = results[\"best_epoch\"]\n",
    "plot_confusion_matrix(results=results, epoch=best_epoch, save_folder=path_CNN_results, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neatly print the most important results.\n",
    "print(f\"Prep time: {results['prep_time']} seconds, prediction time: {results['predict_time']} seconds, (average of {results['predict_time']/10000}) seconds\") \n",
    "      \n",
    "print(f\"Accuracy in best epoch: {results['best_epoch_accuracy']}, Accuracy in final epoch: {results['final_epoch_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, model = load_or_train_CNN(path_CNN_results, path_download, force=False, parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a different look at the average of test time.\n",
    "t_array = [pred_single_image(download_folder=path_download, results_path=path_CNN_results) for i in range(5000)]\n",
    "\n",
    "avg_time = sum(t_array)/len(t_array)\n",
    "avg_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not want to run the above cell because it takes too long, believe us on our blue eyes (dutchism, I know) that it takes at least 1.5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random testing image\n",
    "img = get_single_image(download_folder=path_download, img_index=5)\n",
    "\n",
    "\n",
    "visualize_activations(model=_model,input_image=img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we will go through the final model chosen for this architecture. In a seperate chapter (named optional), some of the old methods will be shown. They are (sometimes), significantly worse, so we do not show their timings, only their accuracies obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_start = time.perf_counter()\n",
    "known_images, known_images_labels = knn.n_centroid_images_each(train['x'], train['y'], 5)\n",
    "setup_end = time.perf_counter()\n",
    "setup_time = setup_end - setup_start\n",
    "print(\"Setup time: \" + str(setup_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "true_values = [test['y'][i] for i in range(4000, 6000)]\n",
    "correctness_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_start = time.perf_counter()\n",
    "for i in range(4000, 6000):\n",
    "    guess = knn.k_nearest_neighbors(known_images, known_images_labels, test['x'][i], 1)\n",
    "    predictions.append(guess)\n",
    "    if (guess == test['y'][i]):\n",
    "        correctness_list.append(1)\n",
    "    else: correctness_list.append(0)\n",
    "exec_end = time.perf_counter()\n",
    "exec_time = exec_end - exec_start\n",
    "print(\"Execution time: \" + str(exec_time))\n",
    "\n",
    "accuracy = sum(correctness_list) / len(correctness_list)\n",
    "print(\"Accuracy is \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(true_values, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = conf_matrix, display_labels = [str(i) for i in range(10)])\n",
    "disp.plot(cmap = plt.cm.Blues)\n",
    "plt.title(\"Clusters, n = 5, Range (4000, 6000)\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing for this section that we do will be time-intensive. So you CAN re-run this next cell, with force=True, to force all the svds to be re-ran, but a pre-ran pickled file will be present to more quickly grab the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop over all (rank 1-9 are omitted because they perform badly) ranks such that we can compute which rank works best\n",
    "array_t1 = []\n",
    "array_t2 =[]\n",
    "accuracy_list = []\n",
    "array_svd = []\n",
    "\n",
    "rank_range = [i for i in range(10,31)]\n",
    "array_t1, array_t2, accuracy_list, array_svd = load_or_run_all_ranks(train, test, rank_range, path_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what rank was best performing\n",
    "max_index = accuracy_list.index(max(accuracy_list))\n",
    "best_rank = rank_range[max_index]\n",
    "\n",
    "print(f\"The best performing rank is {best_rank} with an accuracy of {accuracy_list[max_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array(rank_range)\n",
    "y= np.array(accuracy_list)\n",
    "plt.xticks(rank_range)\n",
    "plt.scatter(x, y, color='blue', alpha=0.5, s = 20,marker = 'x')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array(rank_range)\n",
    "y= np.array(array_t1)\n",
    "plt.scatter(x, y, color='blue', alpha=0.5, s = 20,marker = 'x')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Set up time')\n",
    "plt.xticks(np.arange(min(x), max(x)+2, 2))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array(rank_range)\n",
    "y= np.array(array_t2)\n",
    "plt.scatter(x, y, color='blue', alpha=0.5, s = 20,marker = 'x')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Prediction time')\n",
    "plt.xticks(np.arange(min(x), max(x)+2, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that rank 23 is preferred . The accuracy is best and the computation time is not significantly different from other ranks. With this rank we shall do the computations again to get a more accurate depiction of train / test time. And we shall plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load data, to get a more accurate timing of rank 23 specifically\n",
    "start_time_1 = time.perf_counter()\n",
    "train1, test1 = load_mnist(path_raw_data)\n",
    "\n",
    "r=23  \n",
    "X_train = train1['x']  # Training images\n",
    "y_train = train1['y']  # Training labels\n",
    "\n",
    "X_test = test1['x']  # Test images\n",
    "y_test = test1['y']  # Test labels\n",
    "\n",
    "# Compute SVD for each class, digits from 0-9\n",
    "svd_dict = compute_svd_per_digit(X_train, y_train, r)\n",
    "end_time_1 = time.perf_counter()\n",
    "\n",
    "# Compute time taken.\n",
    "time_1 = end_time_1 - start_time_1\n",
    "print(f\"Ran the SVD operations for rank {r} in {time_1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test images again\n",
    "start_time_2 = time.perf_counter()\n",
    "train2, test2 = load_mnist(path_raw_data)\n",
    "\n",
    "r=23  \n",
    "X_train = train2['x']  # Training images\n",
    "y_train = train2['y']  # Training labels\n",
    "\n",
    "X_test = test2['x']  # Test images\n",
    "y_test = test2['y']  # Test labels\n",
    "predictions = np.array([classify_image(image, svd_dict) for image in X_test])\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "end_time_2 = time.perf_counter()\n",
    "time_2 = end_time_2 - start_time_2\n",
    "\n",
    "print(f\"Accuracy: {accuracy} with rank:{r}, this took {time_2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, predictions, labels=np.arange(10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.arange(10))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f\"Confusion Matrix for Rank {r}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN methods that were tested but werent chosen in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for these three functions in the KNN .py file.\n",
    "\n",
    "# n_quantiled_averages_each\n",
    "\n",
    "# n_quantiled_images_each\n",
    "\n",
    "# n_random_images_each"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
